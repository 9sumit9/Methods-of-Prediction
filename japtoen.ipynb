{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f747ef-dc2c-4305-9be4-01390ddf18b4",
   "metadata": {},
   "source": [
    "## Language Translation (JA-EN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed7751-54ba-49e1-b3ee-c8ca264612ba",
   "metadata": {},
   "source": [
    "Name: Sumit Kumar Sangroula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdfd162-3cea-4936-b4eb-5a484daf429f",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "There are around 7159 spoken languages around the the world as per Ethnologue \"https://www.ethnologue.com/insights/how-many-languages\". \n",
    "Roughly half of these languages are on the verge of extinction. One primary reason for extinction could be globalization. With the advancement \n",
    "of technical era, people from around the world prefer using global languages such as English, French, Spanish and Arabic. These langauages are \n",
    "used by millions of people and international organizations such as UN, EU and AU. These languages have global reach among millions of people, \n",
    "widely used in their native homelands and has also become secondary language in some countries due to cultural differences and for easying administrative works. The attraction of learning these languages have been increasing day by day as it provide limitless opportunities to the \n",
    "people. \n",
    "Some of the benefits of global languages are:\n",
    "* job opportunites around the world\n",
    "* global communication\n",
    "* cultural exchange\n",
    "* travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009d72b-5ee4-4e3b-acd1-4f64dc7fccd6",
   "metadata": {},
   "source": [
    "However, learning them isn't easy. The following are some of the challenges:\n",
    "* Since the grammar and vocabulary are totally different, it takes a lot of time to be fluent\n",
    "* Finding resources and tutors is hard\n",
    "* Could be costly in terms of fees\n",
    "* Learners may/may not find the opportunities to show their language skills if they don't find the right environment/people for conversation\n",
    "\n",
    "Due to these reasons, people in some countries tend to use their own native language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d44d81-967c-4569-900f-0fe1c3799250",
   "metadata": {},
   "source": [
    "### An Introduction of Japanese Language\n",
    "\n",
    "Japanese people have high pride in their history, culture and identity. They prefer Japanese language over any other global languages since they value their history and culture and they have been excellent in preserving them. The language came into existence some 1500 years ago although there is no clear proof of its existence. Unlike English language, Japanese language has 3 writing systems.\n",
    "* Hiragana (ひらがな) - Alphabets that are used for native/ local words. The strokes are somewhat curve in nature.\n",
    "* Katakana (カタカナ) - Alphabets that are mostly used for derived/ foreign words. The strokes are generally straight lines.\n",
    "* Kanji (漢字) - Originally Chinese, these characters are logographic symbols with some modifications, and replaces the Hiragana characters as needed. Each Kanji has specific meaning derived from the objects in nature.\n",
    "\n",
    "Japanese language is one of the most toughest languages in the world. As per FSI Language Difficulty Ranking \"https://www.fsi-language-courses.org/blog/fsi-language-difficulty/\", it falls in Category V along with Chinese (Mandarin), Cantonese, Korean and Arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a530382-7388-4bd0-a85c-fd3129140f4a",
   "metadata": {},
   "source": [
    "#### Why Japanese Language Translation\n",
    "\n",
    "* Global presence in financial industries\n",
    "* Global automobiles and technology reach\n",
    "* Strong cultural values\n",
    "* Complexity (Human translation is expensive, and AI translation often misses the context and the meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06597885-1584-44c3-837c-a2d5951836c0",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619b3577-acff-48b8-87e6-32ecd0df171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #used for data manipulation and analysis\n",
    "import nltk #natural language tool kit\n",
    "from nltk.tokenize import word_tokenize #to tokenize english text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re #to tokenize japanese text\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM #used for pretrained models\n",
    "from transformers import MarianMTModel, MarianTokenizer #pretrained model to convert Japanese text into English\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, Seq2SeqTrainingArguments  #pretrained model to convert Japanese text into English\n",
    "from nltk.translate.bleu_score import corpus_bleu as cb #corpus_bleu for BLEU score evaluation of the text file\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  #sentence bleu for sentence and SmoothingFunction to prevent BLEU collapse \n",
    "import tensorflow as tsf #for deploying deep learning network model\n",
    "from pathlib import Path #to process files\n",
    "import torch #to perfom mathematical operations in multi-dimensional arrays\n",
    "import os #to acess and manipulate the operating system dependent functionalities\n",
    "import sacrebleu #compute BLEU score\n",
    "from datasets import Dataset #to process the data\n",
    "from transformers import M2M100Tokenizer #used for pretrained models\n",
    "from sacrebleu import corpus_bleu #compute BLEU score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d9a2b-ff46-464d-be5b-9104227b71dc",
   "metadata": {},
   "source": [
    "### Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd284df-99ba-4fd5-ae07-a8c7181e9f72",
   "metadata": {},
   "source": [
    "To begin our project, we will first perform language translation using some random Japanese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c87269-8192-4b3b-b908-b179337642ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Japanese text\n",
    "jap_text = \"私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\"\n",
    "#Eng_trans\n",
    "eng_text =\"My name is Sumit Kumar Sangroula. I live in Kathmandu. There are five people in the family. My hobbies are watching movies and reading novels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34f4cf7-90ca-4aac-9897-6071ccfc7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the Japanese text is : 63\n",
      "The length of the English text is : 142\n"
     ]
    }
   ],
   "source": [
    "#checking the length of both texts\n",
    "print(\"The length of the Japanese text is :\", len(jap_text))\n",
    "print(\"The length of the English text is :\", len(eng_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e03d6-ce4b-45d4-9636-86ce1696b27b",
   "metadata": {},
   "source": [
    "##### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2025180f-1d5d-48f7-a9fa-1c164b92c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese tokens: ['私の名前はスミット', '・', 'クマール', '・', 'サングルーラです', '。', 'カトマンズに住んでいます', '。', '家族は5人です', '。', '趣味は映画鑑賞と小説を読むことです', '。']\n",
      "English tokens: ['My', 'name', 'is', 'Sumit', 'Kumar', 'Sangroula', '.', 'I', 'live', 'in', 'Kathmandu', '.', 'There', 'are', 'five', 'people', 'in', 'the', 'family', '.', 'My', 'hobbies', 'are', 'watching', 'movies', 'and', 'reading', 'novels', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing english words\n",
    "eng_tokens = word_tokenize(eng_text)\n",
    "\n",
    "#since word_tokenize doesn't work for japanese text, we will use findall method\n",
    "jap_tokens = re.findall(r'\\w+|[。、・]', jap_text)\n",
    "\n",
    "#Print\n",
    "print(\"Japanese tokens:\", jap_tokens)\n",
    "print(\"English tokens:\", eng_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6433c-0dda-4589-850c-03b6bef03257",
   "metadata": {},
   "source": [
    "##### Printing the no. of tokens and the counts for both texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd61427d-a18e-4a98-a443-cc67be23e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese tokens: ['私の名前はスミット', '・', 'クマール', '・', 'サングルーラです', '。', 'カトマンズに住んでいます', '。', '家族は5人です', '。', '趣味は映画鑑賞と小説を読むことです', '。']\n",
      "Number of Japanese tokens: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Japanese tokens:\", jap_tokens)\n",
    "print(\"Number of Japanese tokens:\", len(jap_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5572cb3-dc07-4218-910f-4b04bbcada54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tokens: ['My', 'name', 'is', 'Sumit', 'Kumar', 'Sangroula', '.', 'I', 'live', 'in', 'Kathmandu', '.', 'There', 'are', 'five', 'people', 'in', 'the', 'family', '.', 'My', 'hobbies', 'are', 'watching', 'movies', 'and', 'reading', 'novels', '.']\n",
      "Number of English tokens: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"English tokens:\", eng_tokens)\n",
    "print(\"Number of English tokens:\", len(eng_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39c077-feff-414d-b33f-703e8a24fd8d",
   "metadata": {},
   "source": [
    "### Translation using pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf2319-b4b7-4521-ae6b-a291950ea8b0",
   "metadata": {},
   "source": [
    "##### Translation using pretrained model 'Helsinki-NLP/opus-mt-ja-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d89151c-af0f-4f61-9196-acd697b2a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Japanese text:\n",
      "私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\n",
      "\n",
      "Reference English text:\n",
      "My name is Sumit Kumar Sangroula. I live in Kathmandu. There are five people in the family. My hobbies are watching movies and reading novels.\n",
      "\n",
      "Translated English text:\n",
      "My name is Smit Kmer Sanguura. I live in the Catmans. I have five family members. My hobby is watching movies and reading novels.\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained model and tokenizer\n",
    "m1_name = \"Helsinki-NLP/opus-mt-ja-en\" #defining pretrained model\n",
    "tor1 = MarianTokenizer.from_pretrained(m1_name) #tokenizer for the pretrained model\n",
    "m1 = MarianMTModel.from_pretrained(m1_name) #choosing model from the pretrained model\n",
    "\n",
    "#Japanese input text\n",
    "jap_text = \"私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\"\n",
    "\n",
    "#Tokenize and translate\n",
    "inputs1 = tor1(jap_text, return_tensors=\"pt\", padding=True, truncation=True) #tokenize the input text\n",
    "trans1 = m1.generate(**inputs1) #translates the genarated tokens of the input text \n",
    "trns_eng1 = tor1.decode(trans1[0], skip_special_tokens=True) #decodes the translated input\n",
    "\n",
    "#Print\n",
    "print(\"Original Japanese text:\")\n",
    "print(jap_text)\n",
    "print(\"\\nReference English text:\")\n",
    "print(eng_text)\n",
    "print(\"\\nTranslated English text:\")\n",
    "print(trns_eng1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964bfcf-88ea-4e85-894d-4e54d30e48a1",
   "metadata": {},
   "source": [
    "We can see that our that the pretrained model could'nt properly translate the Katakana characters since Katakana characters are mostly used for foreign words. The other translation seem fine as the overall meaning of the both the reference text and translated text give same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37345fa-306f-424a-a2ab-1ca26ea53753",
   "metadata": {},
   "source": [
    "BLEU or Bilingual Evaluation is an algorithm developed to measure the correctness of text which is translated from one language to another. It measures the closeness of human translated text with the machine translated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b328b-cb90-4b73-b9a3-2332c4bb20ba",
   "metadata": {},
   "source": [
    "##### BLEU Score Evaluation for 'Helsinki-NLP/opus-mt-ja-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292a6d7c-55a1-4329-97fd-015d20918ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Japanese text: 私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\n",
      "Reference English text: My name is Sumit Kumar Sangroula. I live in Kathmandu. There are five people in the family. My hobbies are watching movies and reading novels.\n",
      "Model Translated text: My name is Smit Kmer Sanguura. I live in the Catmans. I have five family members. My hobby is watching movies and reading novels.\n",
      "\n",
      "BLEU Score: 32.58\n"
     ]
    }
   ],
   "source": [
    "#Referencing original and translated text\n",
    "eng_ref = [[eng_text]]  #List of English text\n",
    "hypotheses = [trns_eng1] #Hypotheses list\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score1 = corpus_bleu(hypotheses, eng_ref).score\n",
    "\n",
    "#Print\n",
    "print(\"Original Japanese text:\", jap_text)\n",
    "print(\"Reference English text:\", eng_text)\n",
    "print(\"Model Translated text:\", trns_eng1)\n",
    "print(f\"\\nBLEU Score: {bleu_score1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda4603-4969-400c-a361-9e4e29e10347",
   "metadata": {},
   "source": [
    "The score shows that the pretained model performed fairly. But a BLEU score of 32.58 may not perform well when we have large text. We will try another pretrained model to check if it performs better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77be655-5168-4cfa-b8f6-d4c08eaefcfa",
   "metadata": {},
   "source": [
    "##### Translation using pretrained model 'facebook/m2m100_1.2B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de62a142-2ef2-44c4-94e9-8dd3b0a77a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Japanese text:\n",
      "私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\n",
      "\n",
      "Reference English text:\n",
      "My name is Sumit Kumar Sangroula. I live in Kathmandu. There are five people in the family. My hobbies are watching movies and reading novels.\n",
      "\n",
      "Translated English text:\n",
      "My name is Smith Kumar Singhula. I live in Kathmandu. My family is five. My hobbies are watching movies and reading novels.\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained model and tokenizer\n",
    "m2_name = \"facebook/m2m100_1.2B\" #defining pretrained model\n",
    "tor2 = M2M100Tokenizer.from_pretrained(m2_name) #tokenizer for the pretrained model\n",
    "m2 = M2M100ForConditionalGeneration.from_pretrained(m2_name) #choosing model from the pretrained model\n",
    " \n",
    "#Set source and target language\n",
    "tor2.src_lang = \"ja\"   #Source language\n",
    "tgt_lang = \"en\"        #Target language\n",
    "\n",
    "#Japanese input text\n",
    "jap_text = \"私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\"\n",
    "\n",
    "#Tokenize and translate with forced BOS token for target language. Forced BOS forces the model to generate the output with BOS token.\n",
    "encoded1 = tor2(jap_text, return_tensors=\"pt\")\n",
    "generated1 = m2.generate(\n",
    "    **encoded1,\n",
    "    forced_bos_token_id=tor2.get_lang_id(tgt_lang)\n",
    ")\n",
    "trns_eng2 = tor2.decode(generated1[0], skip_special_tokens=True)\n",
    "\n",
    "#Print\n",
    "print(\"Original Japanese text:\")\n",
    "print(jap_text)\n",
    "print(\"\\nReference English text:\")\n",
    "print(eng_text)\n",
    "print(\"\\nTranslated English text:\")\n",
    "print(trns_eng2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca058a3-0cf5-4a2a-a6a7-f9114a063ecf",
   "metadata": {},
   "source": [
    "Our second pretrained model somewhat performed better in translating the Katakana characters although there is room for improvement. Considering the word to word translation, The model somehow performed better than the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6750ee0-46a1-4583-92fe-878ce30db78d",
   "metadata": {},
   "source": [
    "##### BLEU Score Evaluation for 'facebook/m2m100_1.2B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b3bd77-0dd6-426f-bb40-6efc39d17ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Japanese: 私の名前はスミット・クマール・サングルーラです。カトマンズに住んでいます。家族は5人です。趣味は映画鑑賞と小説を読むことです。\n",
      "Reference English: My name is Sumit Kumar Sangroula. I live in Kathmandu. There are five people in the family. My hobbies are watching movies and reading novels.\n",
      "Model Translation: My name is Smith Kumar Singhula. I live in Kathmandu. My family is five. My hobbies are watching movies and reading novels.\n",
      "\n",
      "BLEU Score: 53.25\n"
     ]
    }
   ],
   "source": [
    "#Referencing original and translated text\n",
    "eng_ref = [[eng_text]]  #List of English text\n",
    "hypotheses1 = [trns_eng2] #Hypotheses list\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score2 = corpus_bleu(hypotheses1, eng_ref).score\n",
    "\n",
    "#Print\n",
    "print(\"Original Japanese:\", jap_text)\n",
    "print(\"Reference English:\", eng_text)\n",
    "print(\"Model Translation:\", trns_eng2)\n",
    "print(f\"\\nBLEU Score: {bleu_score2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bb80-7e18-4a77-8f2b-e22f78f87a84",
   "metadata": {},
   "source": [
    "In comparison to 'Helsinki-NLP/opus-mt-ja-en', the 'facebook/m2m100_1.2B' model performed better with BLEU score of 53.25 which is good for human understanding. So, we will use this particular model to translate the Japanese text which has already been translated into English. Then, we will compare the BLEU score and perform fine tuning to see if we get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0066b4-3875-4b07-a7fe-70bbfa42a5c9",
   "metadata": {},
   "source": [
    "### Huge Text Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fa152-aa31-490b-8e2f-f83da06476b2",
   "metadata": {},
   "source": [
    "#### Importing the text file\n",
    "\n",
    "For the huge text translation, we will import the file from https://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz. The files are downloaded on local disk in-case the files get corrupted or deleted from the original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "291bfc84-e1d1-444b-a5b8-75fcf5b597cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_source = 'C:/Users/Acer/Desktop/dataset for method of prediction/japanese language data/kftt-data-1.0'  #file source\n",
    "base_path = Path(file_source).parent / 'kftt-data-1.0' / 'data' / 'orig' #base path of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f7276fb-dea1-4f8d-8319-923b1d2922f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JA-ver: 雪舟（せっしゅう、1420年（応永27年）-1506年（永正3年））は号で、15世紀後半室町時代に活躍した水墨画家・禅僧で、画聖とも称えられる。\n",
      "EN-ver: Known as Sesshu (1420 - 1506), he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century, and was called a master painter.\n",
      "\n",
      "JA-ver: 日本の水墨画を一変させた。\n",
      "EN-ver: He revolutionized the Japanese ink painting.\n",
      "\n",
      "JA-ver: 諱は「等楊（とうよう）」、もしくは「拙宗（せっしゅう）」と号した。\n",
      "EN-ver: He was given the posthumous name \"Toyo\" or \"Sesshu (拙宗).\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naming paths for english and japanese text\n",
    "path_en = os.path.join(base_path, 'kyoto-train.en') #path for English text\n",
    "path_ja = os.path.join(base_path, 'kyoto-train.ja') #path for Japanese text\n",
    "\n",
    "#Reading the data using open and readlines method\n",
    "with open(path_en, encoding='utf-8') as f_en, open(path_ja, encoding='utf-8') as f_ja:\n",
    "    ja_sents = f_ja.readlines() #read Japanese text\n",
    "    en_sents = f_en.readlines() #read English text\n",
    "    \n",
    "#Stripping the sentences in the text\n",
    "ja_sents = [line.strip() for line in ja_sents] #strips the Japanese sentences\n",
    "en_sents = [line.strip() for line in en_sents] #strips the English sentences\n",
    "\n",
    "\n",
    "#Printing the sentences\n",
    "for i in range(3): #to print first 3 sentences \n",
    "    print(f\"JA-ver: {ja_sents[i]}\") #prints Japanese sentences\n",
    "    print(f\"EN-ver: {en_sents[i]}\\n\") #prints English sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258f16b-3ba0-413d-8dc3-82caececf830",
   "metadata": {},
   "source": [
    "The output above is based upon the source Japanese text and the already translated English text from the source file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed10d4d-9dbf-46e3-8913-b5e897d5ab58",
   "metadata": {},
   "source": [
    "##### Counting the no. of sentences in both the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3997cb-a4cf-418e-b79e-971e74515bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Japanese sentences: 362720\n",
      "Total number of English sentences: 451747\n"
     ]
    }
   ],
   "source": [
    "#Use punctuation splitting (。！？) to count Japanese sentences since sentence tokenizer doesn't work for Japanese text translation\n",
    "ja_sents_count = sum(len(re.findall(r'[^。！？]+[。！？]', line)) for line in ja_sents)\n",
    "print(f\"Total number of Japanese sentences: {ja_sents_count}\")\n",
    "\n",
    "#Use sentence tokenizer to count the English sentences\n",
    "en_sents_count = sum(len(sent_tokenize(line)) for line in en_sents)\n",
    "print(f\"Total number of English sentences: {en_sents_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9aac5b-6430-4f4b-9a21-ee1cf1e13e4f",
   "metadata": {},
   "source": [
    "### Translation of the source text using pretrained model 'facebook/m2m100_1.2B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c95ca3-3df4-403a-a254-4ae5161d37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JA-ver: 雪舟（せっしゅう、1420年（応永27年）-1506年（永正3年））は号で、15世紀後半室町時代に活躍した水墨画家・禅僧で、画聖とも称えられる。\n",
      "Ref EN-ver: Known as Sesshu (1420 - 1506), he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century, and was called a master painter.\n",
      "Predicted EN-ver : Snowboat (1420 (応永27年)-1506 (永正3年) is a painting monk and painting artist who worked in the late 15th century during the late 15th century.\n",
      "\n",
      "JA-ver: 日本の水墨画を一変させた。\n",
      "Ref EN-ver: He revolutionized the Japanese ink painting.\n",
      "Predicted EN-ver : It changed the Japanese painting.\n",
      "\n",
      "JA-ver: 諱は「等楊（とうよう）」、もしくは「拙宗（せっしゅう）」と号した。\n",
      "Ref EN-ver: He was given the posthumous name \"Toyo\" or \"Sesshu (拙宗).\"\n",
      "Predicted EN-ver : He said, “See, it’s a shame, it’s a shame, it’s a shame.”\n",
      "\n",
      "JA-ver: 備中国に生まれ、京都・相国寺に入ってから周防国に移る。\n",
      "Ref EN-ver: Born in Bicchu Province, he moved to Suo Province after entering SShokoku-ji Temple in Kyoto.\n",
      "Predicted EN-ver : Born in China, he moved to Kyoto after entering the temple.\n",
      "\n",
      "JA-ver: その後遣明使に随行して中国（明）に渡って中国の水墨画を学んだ。\n",
      "Ref EN-ver: Later he accompanied a mission to Ming Dynasty China and learned Chinese ink painting.\n",
      "Predicted EN-ver : Then he followed the Prophet (peace be upon him) and traveled through China to learn Chinese paintings.\n",
      "\n",
      "JA-ver: 作品は数多く、中国風の山水画だけでなく人物画や花鳥画もよくした。\n",
      "Ref EN-ver: His works were many, including not only Chinese-style landscape paintings, but also portraits and pictures of flowers and birds.\n",
      "Predicted EN-ver : There are many works, not only Chinese mountain water paintings, but also characters and bird paintings.\n",
      "\n",
      "JA-ver: 大胆な構図と力強い筆線は非常に個性的な画風を作り出している。\n",
      "Ref EN-ver: His bold compositions and strong brush strokes constituted an extremely distinctive style.\n",
      "Predicted EN-ver : The bold patterns and strong pen lines create a very individual style.\n",
      "\n",
      "JA-ver: 現存する作品のうち6点が国宝に指定されており、日本の画家のなかでも別格の評価を受けているといえる。\n",
      "Ref EN-ver: 6 of his extant works are designated national treasures. Indeed, he is considered to be extraordinary among Japanese painters.\n",
      "Predicted EN-ver : Six of the existing works have been designated as national treasures and are also highly regarded by Japanese painters.\n",
      "\n",
      "JA-ver: このため、花鳥図屏風などに「伝雪舟筆」される作品は大変多い。\n",
      "Ref EN-ver: For this reason, there are a great many artworks that are attributed to him, such as folding screens with pictures of flowers and that birds are painted on them.\n",
      "Predicted EN-ver : For this reason, there are a lot of works that are \"transmitted to the snowboard\" on flowerbird screens.\n",
      "\n",
      "JA-ver: 真筆であるか専門家の間でも意見の分かれるものも多々ある。\n",
      "Ref EN-ver: There are many works that even experts cannot agree if they are really his work or not.\n",
      "Predicted EN-ver : There are many differences of opinion between experts and authors.\n"
     ]
    }
   ],
   "source": [
    "#Trying to use GPU for faster processing\n",
    "pr_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Loading Model and Tokenizer\n",
    "m3_name = \"facebook/m2m100_1.2B\"\n",
    "tor3 = M2M100Tokenizer.from_pretrained(m3_name)\n",
    "m3 = M2M100ForConditionalGeneration.from_pretrained(m3_name).to(pr_device) #choosing model from the pretrained model\n",
    "\n",
    "#Source language is Japanese and the target language is English\n",
    "tor3.src_lang = \"ja\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "#File source\n",
    "file_source = 'C:/Users/Acer/Desktop/dataset for method of prediction/japanese language data/kftt-data-1.0'\n",
    "base_path = Path(file_source).parent / 'kftt-data-1.0' / 'data' / 'orig'\n",
    "path_en = os.path.join(base_path, 'kyoto-train.en')\n",
    "path_ja = os.path.join(base_path, 'kyoto-train.ja')\n",
    "\n",
    "#Reading the file\n",
    "with open(path_en, encoding='utf-8') as f_en, open(path_ja, encoding='utf-8') as f_ja:\n",
    "    en_sentences = [line.strip() for line in f_en.readlines()]\n",
    "    ja_sentences = [line.strip() for line in f_ja.readlines()]\n",
    "\n",
    "#Translate the first 10 sentences from Japanese to English to see if more sentences give high BLEU score\n",
    "for i in range(10):\n",
    "    ja_text = ja_sentences[i]\n",
    "    inputs = tor3(ja_text, return_tensors=\"pt\").to(pr_device)\n",
    "    translated_tokens = m3.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tor3.get_lang_id(target_lang)\n",
    "    )\n",
    "    translated_text = tor3.decode(translated_tokens[0], skip_special_tokens=True) #Predicted Englis text\n",
    "\n",
    "    #Print\n",
    "    print(f\"\\nJA-ver: {ja_text}\") #Prints Japanse version\n",
    "    print(f\"Ref EN-ver: {en_sentences[i]}\") #Prints Reference English version\n",
    "    print(f\"Predicted EN-ver : {translated_text}\") #Prints Predicted English version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa6b40-5230-4b08-9bca-2ae3bb9aed3c",
   "metadata": {},
   "source": [
    "##### BLEU Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d64c34b-cafd-4b9c-b9c2-06253ccd5bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The BLEU score for 10 sentences: 10.47\n"
     ]
    }
   ],
   "source": [
    "# Prepare reference and hypothesis lists\n",
    "sent_references = [[nltk.word_tokenize(en_sentences[i])] for i in range(10)] #sentence reference list which inhabits the already translated 10 English sentences.\n",
    "hypotheses3 = [] #creating an empty list for hypothesis\n",
    "\n",
    "for i in range(10): #to compare the first 10 sentences\n",
    "    ja_text = ja_sentences[i]\n",
    "    inputs = tor3(ja_text, return_tensors=\"pt\").to(pr_device)\n",
    "    translated_tokens = m3.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tor3.get_lang_id(target_lang)\n",
    "    )\n",
    "    translated_text = tor3.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    hypotheses3.append(nltk.word_tokenize(translated_text))\n",
    "\n",
    "#Calculate BLEU score\n",
    "smooth = SmoothingFunction().method4\n",
    "bleu_score3 = cb(sent_references, hypotheses3, smoothing_function=smooth)\n",
    "print(f\"\\nThe BLEU score for 10 sentences: {bleu_score3 * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e9054-e7b4-42ab-8e35-c765e6fa3284",
   "metadata": {},
   "source": [
    "A BLEU score of 10.47 was achieved while translating only 10 sentences. The score was 8.03 for 30 sentences which seems our model didn't do well enough. Since displaying 30 translated sentences occupy more space and translation time is longer; the translated version of the 30 sentences has been uploaded at https://github.com/9sumit9/Big-Data-Analytics/blob/main/JA2EN for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca6274-4aca-4313-8c1d-f43bcd2b6844",
   "metadata": {},
   "source": [
    "### Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a085601-4122-4f2a-9dc4-0a518ee4b6a6",
   "metadata": {},
   "source": [
    "##### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60e451f-e119-482b-a9b1-bd6092200e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import M2M100Tokenizer\n",
    "\n",
    "#Loading Japanese and English sentences\n",
    "data_pairs = [{'text_translation': {'ja': ja, 'en': en}} for ja, en in zip(ja_sentences, en_sentences)]  #Create a list with dictionary for Japanese and Englsih text\n",
    " \n",
    "dataset = Dataset.from_list(data_pairs)\n",
    "tor4 = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075971d-59cb-4447-9ab8-6e563e802ce7",
   "metadata": {},
   "source": [
    "##### Tokenizing the whole textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd0cf5b-f2fd-481c-9ae0-f2f7871054b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42f0a4af5744286b37aa856f4caa5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/440288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Source and target language\n",
    "src_lang = \"ja\"\n",
    "tgt_lang = \"en\"\n",
    "\n",
    "tor4.src_lang = src_lang\n",
    "tor4.tgt_lang = tgt_lang\n",
    "\n",
    "def tokenize(batch): #Creating tokenize function\n",
    "    src_texts = [item[src_lang] for item in batch[\"text_translation\"]]\n",
    "    tgt_texts = [item[tgt_lang] for item in batch[\"text_translation\"]]\n",
    "\n",
    "    inputs = tor4(src_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    with tor4.as_target_tokenizer():\n",
    "        labels = tor4(tgt_texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "toked_dataset = dataset.map(tokenize, batched=True) #mapping the data for both languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012ae26-a7b5-46a8-9efd-bc5ca7a9c883",
   "metadata": {},
   "source": [
    " Mapping takes a long time since there are 362720 Japanese sentences and 451747 English sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91283328-18ec-4b9b-a3c3-f759fe22a36d",
   "metadata": {},
   "source": [
    "##### Model Loading and BLEU score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2956c80-7a99-4f97-aff8-f07e2f50ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The BLEU score for 100 sentences is : 8.50\n"
     ]
    }
   ],
   "source": [
    "#Using SmoothingFunction\n",
    "smooth1 = SmoothingFunction().method4\n",
    "\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "#Translating the first 100 sentences for evaluation\n",
    "for i in range(100):\n",
    "    inputs4 = tor4(ja_sentences[i], return_tensors=\"pt\").to(pr_device)\n",
    "    translated_tokens2 = m3.generate(\n",
    "        **inputs4,\n",
    "        forced_bos_token_id=tor4.get_lang_id(target_lang)\n",
    "    )\n",
    "    pred = tor4.decode(translated_tokens2[0], skip_special_tokens=True)\n",
    "\n",
    "    preds.append(pred.split())\n",
    "    refs.append([en_sentences[i].split()])  #Referring to the list of English sentences\n",
    "\n",
    "#BLEU score evaluation\n",
    "b_score = [sentence_bleu(refs[i], preds[i], smoothing_function=smooth1) for i in range(len(preds))]\n",
    "avg_bleu = sum(b_score) / len(b_score)\n",
    "print(f\"\\nThe BLEU score for {len(preds)} sentences is : {avg_bleu * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961dddf9-6e6f-4d06-9f3f-21565177527b",
   "metadata": {},
   "source": [
    "##### Re_training the model with Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dcaadc5-0951-41b6-a0d3-6ec73be3bdbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer, Seq2SeqTrainingArguments\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#Assigning Seq2Seq trainer\u001b[39;00m\n\u001b[32m      4\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m      5\u001b[39m     model=model,\n\u001b[32m      6\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2154\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2153\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2155\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2184\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2182\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2181\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2183\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2184\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, logging\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecate_kwarg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\trainer.py:42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2154\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2153\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2155\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2184\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2182\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2181\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2183\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2184\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel, TrainingArguments\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     PushToHubMixin,\n\u001b[32m     41\u001b[39m     flatten_dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     logging,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2154\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2153\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2155\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2184\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2182\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2181\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2183\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2184\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "#Assigning Seq2Seq trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset.shuffle(seed=42).select(range(10000)),  #we have set 10000 tokens for train dataset.\n",
    "    eval_dataset=tokenized_dataset.select(range(500)),  #Evaluating 500 tokens\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec54f8-92b7-40e2-ac6f-677044069894",
   "metadata": {},
   "source": [
    "##### BLEU score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0e8d0-4fc5-416e-aa3d-0003de9db181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Translate test sentences\n",
    "def translate_sentences(m3, tor4, input_texts, src_lang=\"ja\", tgt_lang=\"en\"):\n",
    "    tor4.src_lang = src_lang\n",
    "    translated_texts = []\n",
    "    for text in input_texts:\n",
    "        encoded = tor4(text, return_tensors=\"pt\", padding=True, truncation=True).to(m3.device)\n",
    "        generated = m3.generate(**encoded, forced_bos_token_id=tor4.get_lang_id(tgt_lang))\n",
    "        translated = tor4.decode(generated[0], skip_special_tokens=True)\n",
    "        translated_texts.append(translated)\n",
    "    return translated_texts\n",
    "\n",
    "predictions = translate_sentences(m3, tor4, ja_sentences[:100])\n",
    "bleu = corpus_bleu(predictions, [en_sentences[:100]])\n",
    "print(f\"BLEU Score: {bleu.score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbdc59-5f87-4ebf-96a9-820269639676",
   "metadata": {},
   "source": [
    "The output above showed little improvement than the previous output. But still we can't deploy the model for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec5c8d-a955-45dc-8634-c583abd02ab1",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Language translation seems a sublime project, it comes with unlimited challenges. Performing it at an individual level is even harder and challenging given the amount of text you are fed and time you have to spent while training the models and evaluating the output. A model that works for one particular language may not work for other language. \n",
    "\n",
    "Athough there are other highly trained pretrained models available such as 'seamless-m4t-large', 'facebook/nllb-200-distilled-600M', 'facebook/nllb-200-1.3B' for translating the text, there are few drawbacks in translation.\n",
    "\n",
    "Some of the drawbacks for language translation are:\n",
    "* time consumption\n",
    "* have to download huge packages which may not work in every evaluation models\n",
    "* availability of GPU and CPU for processing\n",
    "* patience\n",
    "* no guarantee of perfect BLEU scoreIt requires large amount of CPU and GPU. Also, training the model takes a lot of time. \n",
    "\n",
    "Building our own seqtoseqmodel for translation for a small text would yield a perfect 100% BLEU score since the source text will make comparisions with the reference translated text provided to the model. But in real case scenario, getting a perfect 100% BLEU score is not attainable. For a huge text, manually providing the translated text for reference is tiresome and time consuming. To do so, we need to hire language experts and possess highly powered computing devices since training the data takes a lot of time and requires extra CPU and GPU. So for language translation for huge text, it is advisable to use pretrained model for faster training and evaluation. Normally, a BLEU score of above 50 is considered good translation for human understanding.\n",
    "\n",
    "Conclusively, to get better translation results, using pretrained models developed by huge companies like GOOGLE, Meta and Microsoft at an organizational level would do justice to the translation projects only if the above drawbacks are met."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
